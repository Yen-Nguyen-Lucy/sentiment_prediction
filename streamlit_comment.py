import numpy as np
import pandas as pd
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split  
from sklearn.metrics import accuracy_score 
from sklearn.metrics import confusion_matrix
from sklearn. metrics import classification_report, roc_auc_score, roc_curve
import pickle
import streamlit as st
import matplotlib.pyplot as plt
from sklearn import metrics
import regex
import string
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from underthesea import word_tokenize, pos_tag, sent_tokenize

from wordcloud import WordCloud
from wordcloud import WordCloud, STOPWORDS
import time

import pandas as pd
import re

# 1. Load data
san_pham = pd.read_csv('San_pham.csv')
data = pd.read_csv('Danh_gia.csv', encoding='utf-8')
X_train = pd.read_csv('x_train.csv', index_col=0)
y_train = pd.read_csv('y_train.csv', index_col=0)
X_test = pd.read_csv('x_test.csv', index_col=0)
y_test = pd.read_csv('y_test.csv', index_col=0)
pre_df = pd.read_csv('prepared_4_data.csv', encoding='utf-8')
data_visual = pre_df[['joined_positive_words', 'joined_negative_words']].fillna('').astype(str)

merged_data = pd.merge(data, san_pham[['ma_san_pham', 'ten_san_pham']], on='ma_san_pham', how='left')

# 2. h√†m l√†m s·∫°ch d·ªØ li·ªáu
## ƒê·ªçc file icon v√† words
with open('negative_icon.txt', 'r', encoding='utf-8') as file:
    icon_nev = [icon.strip() for icon in file.read().splitlines() if icon.strip()]

#file icon t√≠ch c·ª±c
with open('positive_icon.txt', 'r', encoding='utf-8') as file:
    icon_pov = [icon.strip() for icon in file.read().splitlines() if icon.strip()]

#file word ti√™u c·ª±c
with open('negative_word.txt', 'r', encoding='utf-8') as file:
    word_nev = [word.strip() for word in file.read().split('\n') if word.strip()]

#file word t√≠ch c·ª±c
with open('positive_word.txt', 'r', encoding='utf-8') as file:
    word_pov = [word.strip() for word in file.read().split('\n') if word.strip()]

#file t·ª´ ƒëi·ªÉn ti·∫øng Vi·ªát
with open('vie_dict.txt', 'r', encoding='utf-8') as file:
    vie_dict = [word.strip() for word in file.read().split('\n') if word.strip()]

#file emoji chuy·ªÉn sang c·∫£m x√∫c b·∫±ng l·ªùi
with open('emojicon_copy.txt', 'r', encoding="utf8") as file:
    emoji_dict = {line.split('\t')[0]: line.split('\t')[1].strip() for line in file if '\t' in line}

#file teen code
with open('teencode.txt', 'r', encoding='utf8') as file:
    teen_dict = {line.split('\t')[0]: line.split('\t')[1].strip() for line in file if '\t' in line}

#file t·ª´ ƒëi·ªÉn Anh-Vi·ªát
with open('english-vnmese.txt', 'r', encoding='utf8') as file:
    english_dict= {line.split('\t')[0]: line.split('\t')[1].strip() for line in file if '\t' in line}

#file wrong word
with open('wrong-word.txt', 'r', encoding='utf8') as file:
    wrong_lst = file.read().split('\n')

#file stopword
with open('vietnamese-stopwords.txt', 'r', encoding='utf8') as file:
    stopwords_lst = file.read().split('\n')



phrases_to_join = [ "d∆∞·ª°ng ·∫©m", "nh·∫°y c·∫£m", "l√†m s·∫°ch", "c·∫•p ·∫©m", "t√°i t·∫°o da",
    'th·∫•m nhanh', 'kh√¥ng ·∫£nh h∆∞·ªüng', 's√°ng da', 's·∫°ch s√¢u',
    'm·ªù th√¢m', 'da d·∫ßu', 'ch·ªëng n·∫Øng', 'tr·ªã m·ª•n',
    'd·ªãu nh·∫π', 'l·ªó ch√¢n l√¥ng', 'h√†ng ng√†y', 'm·ªãn da',
     'ngƒÉn ng·ª´a', 'l√£o h√≥a', 'd·∫°ng n∆∞·ªõc', 'ki·ªÅm d·∫ßu', 'ng·ª´a m·ª•n', 'b·ªçt m·ªãn',
    'gi·∫£m m·ª•n', 'da m·ª•n', 'gi·∫£m nh·ªùn','kh√¥ da',
    'd·ªãu nh·∫π', 'se l·ªó ch√¢n l√¥ng', 't·∫©y t·∫ø b√†o ch·∫øt',
   't·∫©y da ch·∫øt', 'ch·ªëng b·ª•i', 'l·ªôt m·ª•n', 'd√°n m≈©i', 't·∫°o b·ªçt',
    'n√¢ng t√¥ng',  'qu·∫ßng th√¢m', 'b·ªçng m·∫Øt', 'th·∫©m th·∫•u', 'nhanh th·∫•m', 'cay m·∫Øt', 'ch√≠nh h√£ng',  'hi·ªáu qu·∫£', 'h·ªëi h·∫≠n', 'th·∫•t v·ªçng',
    't·ªá h·∫°i', 'kinh kh·ªßng',  'kh√¥ng ∆∞ng √Ω', 'kh√¥ng ƒë∆∞·ª£c t·ªët', 'kinh kh·ªßng', 'm√πi n·ªìng', 'kh√¥ng d√πng ƒë∆∞·ª£c',
    'kh√¥ng n√™n', 'kh√¥ng ƒë·ª°','b·ªã m·ª•n', 'cay m·∫Øt', 'h·ªëi h·∫≠n',  'kh√¥ng x·ª©ng ƒë√°ng', 'ch√¢m ch√≠ch', 'ƒë·ª´ng n√™n',
    'b·∫øt r√≠t', '√¢m ƒëi·ªÉm', 'sai l·∫ßm', 'kh√≥ ch·ªãu', 'kh√¥ da', 'ph√≠ ti·ªÅn', 'k√©m b·ªÅn', 'da s·∫ßn', 'm√πi c·ªìn',
    'b·ª±c m√¨nh', 'kh√¥ng ƒë·∫°t', 'nh·ªùn r√≠t','h·∫øt h·∫°n', 'ph cao', 'v√¥ d·ª•ng', 'kh√¥ng ki·ªÅm d·∫ßu', 'tr·∫Øng b·ªách',
    'kh√≥ th·∫•m', 'd·ªã ·ª©ng', 'gi·∫≠n d·ªØ', 'm·ªát m·ªèi',  'c·∫£i thi·ªán', 's·ªØa r·ª≠a m·∫∑t', 't·∫©y s·∫°ch', 'd·ªãu nh·∫π',
    'ch·ªëng n·∫Øng', 'n√¢ng t√¥ng', 'l√™n t√¥ng', 'hi·ªáu qu·∫£', 'th∆∞ gi√£n']

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from underthesea import word_tokenize

# H√†m x·ª≠ l√Ω vƒÉn b·∫£n
def process_text_pipeline(text, emoji_dict, teen_dict, wrong_lst, stopwords, phrases_to_join):
    # B∆∞·ªõc 1: Chuy·ªÉn vƒÉn b·∫£n v·ªÅ ch·ªØ th∆∞·ªùng
    text = text.lower()

    # B∆∞·ªõc 2: Thay th·∫ø emoji
    text = ''.join(emoji_dict.get(char, char) for char in text)

    # B∆∞·ªõc 3: Thay th·∫ø teen slang
    text = ' '.join(teen_dict.get(word, word) for word in text.split())

    # B∆∞·ªõc 4: Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng mong mu·ªën
    text = ' '.join(word for word in text.split() if word not in wrong_lst)

    # B∆∞·ªõc 5: Gh√©p c√°c c·ª•m t·ª´
    for phrase in phrases_to_join:
        text = text.replace(phrase, '_'.join(phrase.split()))

    # B∆∞·ªõc 6: Lo·∫°i b·ªè stopwords
    text = ' '.join(word for word in text.split() if word not in stopwords)

    # B∆∞·ªõc 7: Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'\s+', ' ', text).strip()

    return text




# H√†m ƒë·∫øm s·ªë l∆∞·ª£ng t·ª´
def count_words(text, word_list):
    # T·∫°o regex ƒë·ªÉ ƒë·∫øm s·ªë l∆∞·ª£ng t·ª´ trong word_list
    pattern = r'\b(?:' + '|'.join(re.escape(word) for word in word_list) + r')\b'
    return len(re.findall(pattern, text))

# H√†m vector h√≥a vƒÉn b·∫£n
def vectorize_text(data, column):
    vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.9)
    train_vec  = vectorizer.fit_transform(pre_df['word_pro3'])
    X = vectorizer.transform(data[column])
    return X, vectorizer



# H√†m x·ª≠ l√Ω v√† d·ª± ƒëo√°n c·∫£m x√∫c
def analyze_file(lines, emoji_dict, teen_dict, wrong_lst, stopwords, phrases_to_join, word_nev, word_pov, model):
    if isinstance(lines, str):
        lines = [lines]
    # T·∫°o DataFrame t·ª´ c√°c d√≤ng
    data = pd.DataFrame({'text': [line.strip() for line in lines]})
    
    # ƒê·∫£m b·∫£o c·ªôt 'text' t·ªìn t·∫°i
    if 'text' not in data.columns:
        raise ValueError("The input data must contain a 'text' column.")

    # B∆∞·ªõc 2: X·ª≠ l√Ω vƒÉn b·∫£n
    data['processed_text'] = data['text'].apply(
        lambda x: process_text_pipeline(x, emoji_dict, teen_dict, wrong_lst, stopwords, phrases_to_join)
    )

    # B∆∞·ªõc 3: ƒê·∫øm t·ª´ ti√™u c·ª±c v√† t√≠ch c·ª±c
    data['negative_count'] = data['processed_text'].apply(lambda x: count_words(x, word_nev))
    data['positive_count'] = data['processed_text'].apply(lambda x: count_words(x, word_pov))

    # B∆∞·ªõc 4: T√≠nh ƒë·ªô d√†i vƒÉn b·∫£n
    data['text_length'] = data['processed_text'].apply(len)

    # B∆∞·ªõc 5: Vector h√≥a vƒÉn b·∫£n
    X_text, vectorizer = vectorize_text(data, 'processed_text')

    # B∆∞·ªõc 6: K·∫øt h·ª£p c√°c ƒë·∫∑c tr∆∞ng
    import numpy as np
    additional_features = data[['negative_count', 'positive_count']].values
    X = np.hstack((X_text.toarray(), additional_features))

    # B∆∞·ªõc 7: D·ª± ƒëo√°n c·∫£m x√∫c
    data['prediction'] = model.predict(X)

    # Tr·∫£ v·ªÅ DataFrame k·∫øt qu·∫£
    return data[['text', 'prediction']]




#1. load model
from joblib import load
loaded_model_random = load('model_random_weight.joblib')

#4. Evaluate model
y_pred = loaded_model_random.predict(X_test)
score_train = loaded_model_random.score(X_train,y_train)
score_test = loaded_model_random.score(X_test,y_test)
acc = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred, labels=[0,1])

cr = classification_report(y_test, y_pred)

y_prob = loaded_model_random.predict_proba(X_test)
roc = roc_auc_score(y_test, y_prob[:,1])


#Product Visualization
def analyze_product_by_code(ma_san_pham):
    # Ki·ªÉm tra m√£ s·∫£n ph·∫©m c√≥ t·ªìn t·∫°i trong h·ªá th·ªëng kh√¥ng
    product_data = san_pham[san_pham['ma_san_pham'] == ma_san_pham]['ten_san_pham'].values

    if not product_data:
        st.write(f"M√£ s·∫£n ph·∫©m '{ma_san_pham}' kh√¥ng t·ªìn t·∫°i trong h·ªá th·ªëng.")
        return

    # L·∫•y t√™n s·∫£n ph·∫©m t·ª´ m√£ s·∫£n ph·∫©m
    product_name = product_data[0]
    st.write(f"T√™n s·∫£n ph·∫©m: {product_name}")

    # L·ªçc d·ªØ li·ªáu nh·∫≠n x√©t t√≠ch c·ª±c v√† ti√™u c·ª±c
    filtered_df_pos = pre_df[(pre_df['star_cleaned'] == 5) & (pre_df['ma_san_pham'] == ma_san_pham)]
    filtered_df_neg = pre_df[(pre_df['star_cleaned'] == 1) & (pre_df['ma_san_pham'] == ma_san_pham)]

    # S·ªë l∆∞·ª£ng nh·∫≠n x√©t t√≠ch c·ª±c v√† ti√™u c·ª±c
    positive_count = len(filtered_df_pos)
    negative_count = len(filtered_df_neg)

    st.write(f'S·ªë l∆∞·ª£ng nh·∫≠n x√©t t√≠ch c·ª±c: {positive_count}')
    st.write(f'S·ªë l∆∞·ª£ng nh·∫≠n x√©t ti√™u c·ª±c: {negative_count}')

    # 1. T√≠nh to√°n s·ªë sao b√¨nh qu√¢n
    avg_rating = pre_df[pre_df['ma_san_pham'] == ma_san_pham]['star_cleaned'].mean()
    st.write(f"S·ªë sao b√¨nh qu√¢n: {avg_rating:.2f}")

    # 2. Th·ªùi gian c·ªßa c√°c b√¨nh lu·∫≠n
    # ƒê·∫£m b·∫£o chuy·ªÉn ƒë·ªïi ng√†y th√°ng ƒë√∫ng ƒë·ªãnh d·∫°ng
    filtered_df_pos['ngay_binh_luan'] = pd.to_datetime(filtered_df_pos['ngay_binh_luan'], dayfirst=True, errors='coerce')
    filtered_df_neg['ngay_binh_luan'] = pd.to_datetime(filtered_df_neg['ngay_binh_luan'], dayfirst=True, errors='coerce')

    # Lo·∫°i b·ªè gi√° tr·ªã NaT
    filtered_df_pos = filtered_df_pos.dropna(subset=['ngay_binh_luan'])
    filtered_df_neg = filtered_df_neg.dropna(subset=['ngay_binh_luan'])

    # T·∫°o danh s√°ch ng√†y b√¨nh lu·∫≠n
    comment_dates = (
        pd.to_datetime(filtered_df_pos['ngay_binh_luan']).tolist() +
        pd.to_datetime(filtered_df_neg['ngay_binh_luan']).tolist()
    )

    # Ki·ªÉm tra danh s√°ch kh√¥ng r·ªóng tr∆∞·ªõc khi l·∫•y min/max
    if comment_dates:
        st.write(f"Th·ªùi gian b√¨nh lu·∫≠n: {min(comment_dates).strftime('%Y-%m-%d')} ƒë·∫øn {max(comment_dates).strftime('%Y-%m-%d')}")
    else:
        st.write("Kh√¥ng c√≥ d·ªØ li·ªáu ng√†y b√¨nh lu·∫≠n h·ª£p l·ªá.")


    # 3. T√≠nh to√°n s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo gi·ªù trong ng√†y
    filtered_df = pre_df[pre_df['ma_san_pham'] == ma_san_pham]
    filtered_df['gio_binh_luan'] = pd.to_datetime(filtered_df['gio_binh_luan'])
    hour_counts = filtered_df.groupby(filtered_df['gio_binh_luan'].dt.hour).size().reset_index(name='count')

    # V·∫Ω bi·ªÉu ƒë·ªì
    fig, ax = plt.subplots(figsize=(10, 6))  # T·∫°o fig v√† ax
    ax.bar(hour_counts['gio_binh_luan'], hour_counts['count'], color='skyblue')
    ax.set_xlabel('Gi·ªù trong ng√†y', fontsize=14)
    ax.set_ylabel('S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n', fontsize=14)
    ax.set_title(f'S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo gi·ªù trong ng√†y', fontsize=18,fontweight='bold')
    ax.set_xticks(hour_counts['gio_binh_luan'])
    ax.tick_params(axis='x', rotation=45)
    ax.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    st.pyplot(fig)  # Hi·ªÉn th·ªã fig

    # 4. T·∫°o WordCloud cho c√°c nh·∫≠n x√©t t√≠ch c·ª±c v√† ti√™u c·ª±c
    def create_wordcloud(df, sentiment):
        comments = " ".join(df['joined_positive_words'].dropna()) if sentiment == "positive" else " ".join(df['joined_negative_words'].dropna())
        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(comments)
        fig, ax = plt.subplots(figsize=(10, 5))  # T·∫°o fig v√† ax
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')
        ax.set_title(f"WordCloud cho nh·∫≠n x√©t {sentiment}", fontsize=18, fontweight='bold')  
        st.pyplot(fig)  # Hi·ªÉn th·ªã fig

    # Hi·ªÉn th·ªã wordcloud cho nh·∫≠n x√©t t√≠ch c·ª±c v√† ti√™u c·ª±c
    create_wordcloud(filtered_df_pos, "positive")
    create_wordcloud(filtered_df_neg, "negative")




#GUI
st.title("Data Science Project")
st.write("## Sentiment Analysis")

menu = ["Project Info",  "Build Model", "New Prediction"]
# Sidebar Menu
choice = st.sidebar.selectbox('Menu', menu)
# Sidebar Content
st.sidebar.markdown(
    """
    ### **Th√¥ng tin d·ª± √°n**
    - **Th√†nh vi√™n th·ª±c hi·ªán**
      - üü¢ **Phan Th·ªã Thu H·∫°nh**
      - üü° **Nguy·ªÖn H·∫£i Y·∫øn**
    
    - **Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n**
      - üü£ **C√¥ Khu·∫•t Th√πy Ph∆∞∆°ng**
    
    - **Th·ªùi gian b√°o c√°o**
      - üìÖ **14/12/2024**
    """,
    unsafe_allow_html=True
)


if choice == 'Project Info':
    tab1, tab2, tab3, tab4 = st.tabs(['Business Overall', 'Product Info', 'Algorithms Info', 'About Us'])
    with tab1:
        st.image('hasaki.jpg')
        st.subheader('Hasaki - chu·ªói h·ªá th·ªëng ph√¢n ph·ªëi m·ªπ ph·∫©m v·ªõi h∆°n 500 th∆∞∆°ng hi·ªáu')

        st.write(""" Hasaki l√† m·ªôt chu·ªói h·ªá th·ªëng ph√¢n ph·ªëi m·ªπ ph·∫©m ƒëang ƒë∆∞·ª£c ph√°t tri·ªÉn t·∫°i Vi·ªát Nam. Doanh nghi·ªáp ƒëang ng√†y c√†ng m·ªü r·ªông tr·∫£i d√†i kh·∫Øp to√†n qu·ªëc. B√™n c·∫°nh ƒë√≥, Hasaki ƒëang ch√∫ tr·ªçng n√¢ng c·∫•p ph√°t tri·ªÉn h·ªá th·ªëng b√°n h√†ng tr·ª±c tuy·∫øn hasaki.vn ƒë·ªÉ tƒÉng tr·∫£i nghi·ªám ng∆∞·ªùi d√πng. M·ªói ng√†y Hasaki nh·∫≠n v·ªÅ h√†ng ng√†n l∆∞·ª£t ƒë√°nh gi√° v√† t∆∞∆°ng t√°c c·ªßa kh√°ch h√†ng ƒë·ªëi v·ªõi s·∫£n ph·∫©m c·ªßa m√¨nh.
                """)
        
        #t·∫°o 2 h√¨nh
        col1, col2 = st.columns(2)
        with col1:
            st.image('tab1_2.png', caption='·ª®ng d·ª•ng mua s·∫Øm tr·ª±c tuy·∫øn hasaki.vn', use_container_width=True)
        with col2:
            st.image('tab1_1.png', caption='H·ªá th·ªëng m·ªπ ph·∫©m 76 chi nh√°nh tr·∫£i d√†i 27 t·ªânh th√†nh', use_container_width=True)
        st.write("""Vi·ªác qu·∫£n l√Ω v√† thu th·∫≠p √Ω ki·∫øn ƒë√°nh gi√° t·ª´ kh√°ch h√†ng l√† y·∫øu t·ªë then ch·ªët trong vi·ªác c·∫£i thi·ªán s·∫£n ph·∫©m, m·ªü r·ªông t·ªáp kh√°ch h√†ng v√† x√¢y d·ª±ng th∆∞∆°ng hi·ªáu.
Nh·ªù v√†o h·ªá th·ªëng tr·ª±c tuy·∫øn, Hasaki c√≥ th·ªÉ thu th·∫≠p √Ω ki·∫øn m·ªôt c√°ch kh√°ch quan t·ª´ ng∆∞·ªùi d√πng th·ª±c t·∫ø. 
                 Nh·ªØng d·ªØ li·ªáu gi√° tr·ªã n√†y gi√∫p doanh nghi·ªáp ph√¢n t√≠ch s√¢u h∆°n, t·ª´ ƒë√≥ ph√°t tri·ªÉn h·ªá th·ªëng d·ª± ƒëo√°n t√¢m l√Ω kh√°ch h√†ng trong t∆∞∆°ng lai m·ªôt c√°ch ch√≠nh x√°c v√† hi·ªáu qu·∫£ h∆°n.""")
        st.image('tab1_3.png', caption='M·ªói ng√†y hasaki.vn thu v·ªÅ h√†ng ng√†n l∆∞·ª£t t∆∞∆°ng t√°c t·ª´ kh√°ch h√†ng')

        st.write("##### Ph∆∞∆°ng √°n ƒë·ªÅ xu·∫•t")
        st.write('Ph√°t tri·ªÉn h·ªá th·ªëng d·ª± ƒëo√°n t√¢m t∆∞ kh√°ch h√†ng th√¥ng qua nh·ªØng d·ªØ li·ªáu ƒë√°nh gi√° hi·ªán c√≥ b·∫±ng Machine Learning.')

    with tab2:
        st.image('tab2_1.png')
        st.subheader("Kh√°m ph√° s·∫£n ph·∫©m th√¥ng qua t∆∞∆°ng t√°c t·ª´ kh√°ch h√†ng")
        st.write(""" Ph√¢n t√≠ch c·∫£m x√∫c c·ªßa kh√°ch h√†ng th√¥ng qua nh·ªØng b√¨nh lu·∫≠n v·ªÅ s·∫£n ph·∫©m 
                l√† m·ªôt trong nh·ªØng c√°ch gi√∫p cho HASAKI hi·ªÉu r√µ h∆°n v·ªÅ kh√°ch h√†ng c·ªßa m√¨nh 
                c≈©ng nh∆∞ nh·ªØng ƒë√°nh gi√° kh√°ch quan nh·∫•t v√† ch√≠nh x√°c nh·∫•t cho s·∫£n ph·∫©m c·ªßa m√¨nh 
                th√¥ng qua ng∆∞·ªùi d√πng th·ª±c t·∫ø. T·ª´ ƒë√≥ doanh nghi·ªáp c√≥ th·ªÉ c√≥ nh·ªØng k·∫ø ho·∫°ch
                ph√°t tri·ªÉn s·∫£n ph·∫©m c≈©ng nh∆∞ m·ªü r·ªông t·ªáp kh√°ch h√†ng hi·ªán c√≥ c·ªßa m√¨nh""")
        col1, col2= st.columns(2)
        with col1:
            # V·∫Ω bi·ªÉu ƒë·ªì
            top_10_products = merged_data['ten_san_pham'].value_counts().nlargest(10)
            # C·∫Øt ng·∫Øn t√™n s·∫£n ph·∫©m n·∫øu c·∫ßn (gi·ªõi h·∫°n 20 k√Ω t·ª±)
            top_10_products.index = top_10_products.index.str.slice(0, 20)
            # T·∫°o bi·ªÉu ƒë·ªì
            fig, ax = plt.subplots(figsize=(12, 5))
            top_10_products.plot(kind='bar', color='purple', ax=ax)
            # Thi·∫øt l·∫≠p nh√£n v√† ti√™u ƒë·ªÅ
            ax.set_xlabel('T√™n s·∫£n ph·∫©m')
            ax.set_ylabel('S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n')
            ax.set_title('Top 10 s·∫£n ph·∫©m c√≥ nhi·ªÅu b√¨nh lu·∫≠n nh·∫•t')
            # Xoay nh√£n tr·ª•c X
            plt.xticks(rotation=45, ha='right')
            # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì tr√™n Streamlit
            st.pyplot(fig)

        with col2:

            # V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë c·ªßa s·ªë sao (star_cleaned)
            fig, ax = plt.subplots(figsize=(8, 5))
            colors = ['#66c2a5', '#fc8d62', '#8da0cb', '#e78ac3', '#a6d854']  # Gradient xanh - cam - t√≠m
            # S·ªë sao v√† s·ªë l∆∞·ª£ng
            star_counts = data['so_sao'].value_counts().sort_index()
            star_counts.plot(kind='bar', color=colors[:len(star_counts)], ax=ax)
            # Thi·∫øt l·∫≠p nh√£n v√† ti√™u ƒë·ªÅ
            ax.set_xlabel('S·ªë sao ƒë√°nh gi√°')
            ax.set_ylabel('S·ªë l∆∞·ª£ng')
            ax.set_title('Ph√¢n b·ªë c·ªßa s·ªë sao ƒë√°nh gi√°')
            # C·ªë ƒë·ªãnh nh√£n tr·ª•c X
            ax.set_xticks(range(len(star_counts)))
            ax.set_xticklabels(star_counts.index, rotation=0)
            # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì tr√™n Streamlit
            st.pyplot(fig)

        col3, col4 = st.columns(2)
        with col3:
            # V·∫Ω bi·ªÉu ƒë·ªì th·ªùi gian b√¨nh lu·∫≠n
            data['ngay_binh_luan'] = pd.to_datetime(data['ngay_binh_luan'], errors='coerce')
            # Lo·∫°i b·ªè gi√° tr·ªã thi·∫øu
            danh_gia = data.dropna(subset=['ngay_binh_luan'])
            # T·∫°o bi·ªÉu ƒë·ªì
            fig, ax = plt.subplots(figsize=(15, 5))
            danh_gia['ngay_binh_luan'].dt.date.value_counts().sort_index().plot(kind='line', color='orange', ax=ax)
            # Thi·∫øt l·∫≠p nh√£n v√† ti√™u ƒë·ªÅ
            ax.set_xlabel('Ng√†y b√¨nh lu·∫≠n')
            ax.set_ylabel('S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n')
            ax.set_title('T·∫ßn su·∫•t b√¨nh lu·∫≠n theo ng√†y')

            # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì tr√™n Streamlit
            st.pyplot(fig)

        with col4:
            #V·∫Ω bi·ªÉu ƒë·ªì b√¨nh lu·∫≠n theo gi·ªù
            danh_gia['gio_binh_luan'] = pd.to_datetime(danh_gia['gio_binh_luan'], format='%H:%M', errors='coerce')
            # Lo·∫°i b·ªè gi√° tr·ªã thi·∫øu
            danh_gia = danh_gia.dropna(subset=['gio_binh_luan'])
            # T·∫°o bi·ªÉu ƒë·ªì
            fig, ax = plt.subplots(figsize=(10, 5))
            danh_gia['gio_binh_luan'].dt.hour.value_counts().sort_index().plot(kind='bar', color='green', ax=ax)
            # Thi·∫øt l·∫≠p nh√£n v√† ti√™u ƒë·ªÅ
            ax.set_xlabel('Gi·ªù b√¨nh lu·∫≠n')
            ax.set_ylabel('S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n')
            ax.set_title('T·∫ßn su·∫•t b√¨nh lu·∫≠n theo gi·ªù trong ng√†y')

            # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì tr√™n Streamlit
            st.pyplot(fig)


        st.subheader('Ph√¢n t√≠ch n·ªôi dung b√¨nh lu·∫≠n')
        st.write('Hi·ªÉu r√µ h∆°n n·ªôi dung c√°c b√¨nh lu·∫≠n th√¥ng qua vi·ªác hi·ªÉn th·ªã c√°c t·ª´ kh√≥a c√≥ √Ω nghƒ©a xu·∫•t hi·ªán ph·ªï bi·∫øn nh·∫•t trong t·∫•t c·∫£ c√°c b√¨nh lu·∫≠n')

        # Tr·ª±c quan n·ªôi dung t·ª´ kh√≥a s·∫£n ph·∫©m
        text = " ".join(pre_df['word_pro3'])
        # T·∫°o WordCloud
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            stopwords=STOPWORDS,
            colormap='viridis'
        ).generate(text)
        # Hi·ªÉn th·ªã WordCloud
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')  # T·∫Øt tr·ª•c t·ªça ƒë·ªô

        # Hi·ªÉn th·ªã WordCloud tr√™n Streamlit
        st.pyplot(fig)

        #Tr·ª±c quan t·ª´ kh√≥a t√≠ch c·ª±c, ti√™u c·ª±c
        col5, col6 = st.columns(2)

        with col5:
            
            text = " ".join(data_visual['joined_positive_words'])
            # T·∫°o WordCloud
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                stopwords=STOPWORDS,
                colormap='viridis'
            ).generate(text)
            st.write("**Bi·ªÉu ƒë·ªì WordCloud c·ªßa c√°c t·ª´ t√≠ch c·ª±c**")
            # Hi·ªÉn th·ªã WordCloud
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')  # T·∫Øt tr·ª•c t·ªça ƒë·ªô

            # Hi·ªÉn th·ªã WordCloud tr√™n Streamlit
            st.pyplot(fig)


        with col6:
            text = " ".join(data_visual['joined_negative_words'].astype(str))
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color='white',
                stopwords=STOPWORDS,
                colormap='viridis'
            ).generate(text)

            # Ti√™u ƒë·ªÅ cho WordCloud
            st.write("**Bi·ªÉu ƒë·ªì WordCloud c·ªßa c√°c T·ª´ Ti√™u C·ª±c**")

            # Hi·ªÉn th·ªã WordCloud
            fig, ax = plt.subplots(figsize=(10, 5))
            ax.imshow(wordcloud, interpolation='bilinear')
            ax.axis('off')  # T·∫Øt tr·ª•c t·ªça ƒë·ªô

            # Hi·ªÉn th·ªã WordCloud tr√™n Streamlit
            st.pyplot(fig)


        st.subheader('Kh√°m ph√° th√¥ng tin chi ti·∫øt t·ª´ng s·∫£n ph·∫©m')
        # T·∫°o text_input ƒë·ªÉ nh·∫≠p li·ªáu
        ma_san_pham_input = st.text_input("Nh·∫≠p m√£ s·∫£n ph·∫©m:")

        # T·∫°o selectbox v·ªõi danh s√°ch m√£ s·∫£n ph·∫©m
        ma_san_pham_options = san_pham['ma_san_pham'].unique().tolist()
        ma_san_pham_select = st.selectbox("Ho·∫∑c ch·ªçn t·ª´ danh s√°ch:", ma_san_pham_options)

        # Ki·ªÉm tra xem ng∆∞·ªùi d√πng ƒë√£ nh·∫≠p li·ªáu hay ch·ªçn t·ª´ danh s√°ch
        if ma_san_pham_input:
            ma_san_pham = int(ma_san_pham_input)
            st.write("B·∫°n ƒë√£ nh·∫≠p:", ma_san_pham)
        else:
            ma_san_pham = ma_san_pham_select
            st.write("B·∫°n ƒë√£ ch·ªçn:", ma_san_pham)

        # N√∫t "Ph√¢n t√≠ch"
        if st.button("Ph√¢n t√≠ch"):
            if ma_san_pham_input or ma_san_pham_select:  # Ki·ªÉm tra c·∫£ hai tr∆∞·ªùng h·ª£p
                # G·ªçi h√†m ph√¢n t√≠ch
                analyze_product_by_code(ma_san_pham)
            else:
                st.warning("Vui l√≤ng nh·∫≠p m√£ s·∫£n ph·∫©m.")
        
    with tab3:
        st.image('tab3_2.png')
        st.subheader('Random Forest - Thu·∫≠t to√°n x√¢y d·ª±ng m√¥ h√¨nh trong Machine Learning')
        st.write('Random Forest l√† m·ªôt thu·∫≠t to√°n h·ªçc m√°y m·∫°nh m·∫Ω v√† ph·ªï bi·∫øn thu·ªôc nh√≥m ensemble learning (h·ªçc k·∫øt h·ª£p). ƒê√¢y l√† m·ªôt thu·∫≠t to√°n ph√¢n lo·∫°i v√† h·ªìi quy ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Leo Breiman v√†o nƒÉm 2001. Random Forest s·ª≠ d·ª•ng nhi·ªÅu c√¢y quy·∫øt ƒë·ªãnh (decision trees) ƒë·ªÉ t·∫°o ra m·ªôt m√¥ h√¨nh m·∫°nh m·∫Ω h∆°n v√† gi·∫£m thi·ªÉu hi·ªán t∆∞·ª£ng overfitting (m√¥ h√¨nh qu√° kh·ªõp v·ªõi d·ªØ li·ªáu hu·∫•n luy·ªán)')
        st.image('tab3_4.png')
        st.write('## **So s√°nh ƒë·ªô ch√≠nh x√°c v√† th·ªùi gian d·ª± ƒëo√°n so v·ªõi c√°c m√¥ h√¨nh kh√°c**')
        col1, col2 = st.columns(2)
        with col1:
            st.image('tab3_model_compare1.png')
            st.image('tab3_model_compare2.png')
        with col2:
            st.image('tab3_model_compare3.png',width=300)
            st.image('tab3_model_compare4.png', width=300)

        st.subheader('Random Forest l√† thu·∫≠t to√°n ph√π h·ª£p ƒë·ªÉ x√¢y d·ª±ng m√¥ h√¨nh')
        st.write("""
                 1. B·ªô d·ªØ li·ªáu train s·ª≠ d·ª•ng kh√¥ng qu√° l·ªõn\n
                 2. M√¥ h√¨nh c√≥ ƒë·ªô ch√≠nh x√°c cao v√† th·ªùi gian x·ª≠ l√Ω nhanh\n
                 3. M√¥ h√¨nh gi·∫£m overfitting\n
                 4. Kh·∫£ nƒÉng ch·ªëng nhi·ªÖu t·ªët 
""")
    with tab4:
        st.subheader('ƒê·ªì √Ån T·ªët Nghi·ªáp Data Science & Machine Learning')
        st.write('**Trung T√¢m Tin H·ªçc - Tr∆∞·ªùng ƒê·∫°i H·ªçc Khoa H·ªçc T·ª± Nhi√™n**')
        st.write('Th·ªùi gian b√°o c√°o: 14/12/2024')
        st.write("Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n: C√¥ Khu·∫•t Th√πy Ph∆∞∆°ng")
        st.write("""H·ªçc vi√™n th·ª±c hi·ªán: Phan Th·ªã Thu H·∫°nh & Nguy·ªÖn H·∫£i Y·∫øn""")
        st.write('Nh√≥m: Nh√≥m 4')
        st.write('Kh√≥a: DL07_K299')
        st.markdown("---")
        st.write('## Th√¥ng tin h·ªçc vi√™n')
        col1, col2 = st.columns(2)
        with col1:
            st.image('tab4_4.png', caption='H·ªçc vi√™n: Phan Th·ªã Thu H·∫°nh')
        with col2:
            st.image('tab4_2.png', caption='H·ªçc vi√™n: Nguy·ªÖn H·∫£i Y·∫øn')
        st.markdown("---")
        st.markdown("*H·ªçc vi√™n ch√∫ng em xin g·ª≠i l·ªùi c·∫£m ∆°n tr√¢n tr·ªçng nh·∫•t ƒë·∫øn C√¥ Khu·∫•t Th√πy Ph∆∞∆°ng ƒë√£ t·∫≠n t√¨nh h∆∞·ªõng d·∫´n ƒë·ªÉ ch√∫ng em c√≥ th·ªÉ ho√†n th√†nh ƒê·ªì √°n t·ªët nghi·ªáp n√†y!*")
            



elif choice== 'Build Model':
    st.subheader('X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n c·∫£m x√∫c kh√°ch h√†ng v·ªõi thu·∫≠t to√°n Random Forest')

    st.write('#### 1. T·ªïng quan d·ªØ li·ªáu')
    st.write('D·ªØ li·ªáu ƒë∆∞·ª£c ghi l·∫°i tr·ª±c ti·∫øp t·ª´ trang b√°n h√†ng tr·ª±c tuy·∫øn Hasaki.vn, n·ªôi dung b√¨nh lu·∫≠n v√† s·ªë sao ƒë∆∞·ª£c ƒë√°nh gi√° l√† hai d·ªØ li·ªáu quan tr·ªçng ƒë∆∞·ª£c s·ª≠ d·ª•ng ch√≠nh trong vi·ªác x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n.')
    st.dataframe(data[['noi_dung_binh_luan', 'so_sao']].head(3))
    st.dataframe(data[['noi_dung_binh_luan', 'so_sao']].tail(3))
    # V·∫Ω bi·ªÉu ƒë·ªì ph√¢n b·ªë c·ªßa s·ªë sao (star_cleaned
    st.dataframe(data['so_sao'].value_counts())

    st.write('C√°c th∆∞ vi·ªán s·ª≠ d·ª•ng ƒë·ªÉ l√†m s·∫°ch d·ªØ li·ªáu')
    st.code('''
            import pandas as pd
            import regex
            import string
            from underthesea import word_tokenize, pos_tag, sent_tokenize
            from wordcloud import WordCloud, STOPWORDS
            import time
            import re''')
    

    st.write('D·ªØ li·ªáu b√¨nh lu·∫≠n ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, d·ªØ li·ªáu ƒë√°nh gi√° s·ªë sao ƒë∆∞·ª£c chuy·ªÉn th√†nh 2 lo·∫°i 1 sao v√† 5 sao')
    st.dataframe(pre_df[['word_pro3', 'star_cleaned']].head(3))
    st.dataframe(pre_df[['word_pro3', 'star_cleaned']].tail(3))
    st.dataframe(pre_df['star_cleaned'].value_counts())

    st.write('S·ª≠ d·ª•ng TF-IDF ƒë·ªÉ m√£ h√≥a b·ªô d·ªØ li·ªáu')
    st.code('''def vectorize_text(data, column):
                vectorizer = TfidfVectorizer(max_features=100, min_df=2, max_df=0.9)
                train_vec  = vectorizer.fit_transform(pre_df['word_pro3'])
                X = vectorizer.transform(data[column])
                return X, vectorizer''')
    
    
    st.write('##### 2. Visualize Positive & Negative')
    #Tr·ª±c quan t·ª´ kh√≥a t√≠ch c·ª±c, ti√™u c·ª±c
    col5, col6 = st.columns(2)

    with col5:
        
        text = " ".join(data_visual['joined_positive_words'])
        # T·∫°o WordCloud
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            stopwords=STOPWORDS,
            colormap='viridis'
        ).generate(text)
        st.write("**Bi·ªÉu ƒë·ªì WordCloud c·ªßa c√°c t·ª´ t√≠ch c·ª±c**")
        # Hi·ªÉn th·ªã WordCloud
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')  # T·∫Øt tr·ª•c t·ªça ƒë·ªô

        # Hi·ªÉn th·ªã WordCloud tr√™n Streamlit
        st.pyplot(fig)


    with col6:
        text = " ".join(data_visual['joined_negative_words'].astype(str))
        wordcloud = WordCloud(
            width=800,
            height=400,
            background_color='white',
            stopwords=STOPWORDS,
            colormap='viridis'
        ).generate(text)
        st.write("**Bi·ªÉu ƒë·ªì WordCloud c·ªßa c√°c t·ª´ ti√™u c·ª±c**")
        # Hi·ªÉn th·ªã WordCloud
        fig, ax = plt.subplots(figsize=(10, 5))
        ax.imshow(wordcloud, interpolation='bilinear')
        ax.axis('off')  # T·∫Øt tr·ª•c t·ªça ƒë·ªô
        st.pyplot(fig)


    st.write('##### 3. X√¢y d·ª±ng m√¥ h√¨nh v·ªõi thu·∫≠t to√°n Random Forest')
    st.code('from sklearn.ensemble import RandomForestClassifier')
    st.write('Do d·ªØ li·ªáu b·ªã m·∫•t c√¢n b·∫±ng, n√™n khi x√¢y d·ª±ng m√¥ h√¨nh c·∫ßn ƒëi·ªÅu ch·ªânh l·∫°i tr·ªçng s·ªë')
    st.code('''
            class_weights = {1: 8, 5: 1}  # TƒÉng tr·ªçng s·ªë l·ªõp 1
            model_random = RandomForestClassifier(class_weight = class_weights, random_state = 42)
            ''')

    st.write('##### 4. ƒê√°nh gi√° m√¥ h√¨nh')
    st.code("Score train:" + str(round(score_train,2)) + " v√† Score Test:" + str(round(score_test,2)))
    st.code('Accuracy:' + str(round(acc,2)))

    st.write('##### confusion matrix: ')
    st.image("confusion_matrix.png", width=500)
    st.write('''
             T·ªïng s·ªë d·ª± ƒëo√°n ch√≠nh x√°c: 236 + 3491 = 3727.\n
             T·ªïng s·ªë d·ª± ƒëo√°n sai: 31 + 159 = 190.\n
             T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng tr√™n t·ªïng d·ª± ƒëo√°n = 95.17%\n
             V·ªõi s·ªë li·ªáu tr√™n cho th·∫•y m√¥ h√¨nh d·ª± ƒëo√°n t·ªët ƒë·∫∑c bi·ªát ·ªü l·ªõp 5, ma tr·∫≠n nh·∫ßm l·ªõp 1 sang l·ªõp 5 kh√¥ng qu√° cao, c√≥ th·ªÉ ch·∫•p nh·∫≠n ƒë∆∞·ª£c.''')
    st.write('##### Classification report:')
    st.code(cr)

    #calculate roc curve
    st.write('##### ROC curve')
    st.image("roc.png", width=500)
    st.code('ROC AUC score:' + str(round(roc,2)))
    st.write('V·ªõi gi√° tr·ªã 0.97 r·∫•t g·∫ßn v·ªõi gi√° tr·ªã t·ªëi ƒëa l√† 1.0, nghƒ©a l√† m√¥ h√¨nh c√≥ kh·∫£ nƒÉng ph√¢n bi·ªát gi·ªØa c√°c l·ªõp t√≠ch c·ª±c (positive) v√† ti√™u c·ª±c (negative) m·ªôt c√°ch ch√≠nh x√°c trong ph·∫ßn l·ªõn c√°c tr∆∞·ªùng h·ª£p')


    st.write('##### 5. K·∫øt lu·∫≠n:')
    st.write('M√¥ h√¨nh d·ª± ƒëo√°n s·ª≠ d·ª•ng thu·∫≠t to√°n Random Forest c·ªßa Machine Learning ƒë√£ ƒë∆∞·ª£c x√¢y d·ª±ng v√† s·∫µn s√†ng ƒë∆∞a v√†o s·ª≠ d·ª•ng')

elif choice == 'New Prediction':

    st.subheader('D·ª± ƒëo√°n c·∫£m x√∫c kh√°ch h√†ng v·ªõi m√¥ h√¨nh Machine Learning - Thu·∫≠t to√°n Random Forest')
    flag = False
    lines = None
    # Ch·ªçn c√°ch nh·∫≠p d·ªØ li·ªáu
    type = st.radio('Upload data or Input data?', options=('Upload', 'Input'))

    if type == 'Upload':
        # Upload file
        upload_file_1 = st.file_uploader('Choose a file:', type=['txt', 'csv'])
        if upload_file_1 is not None:
            lines = pd.read_csv(upload_file_1, header=None)
            st.dataframe(lines)  # Hi·ªÉn th·ªã n·ªôi dung file ƒë√£ upload
            lines = lines[0]  # L·∫•y c·ªôt ƒë·∫ßu ti√™n l√†m n·ªôi dung d·ª± ƒëo√°n
            flag = True

    elif type == 'Input':
    # Nh·∫≠p tr·ª±c ti·∫øp n·ªôi dung
        lines = st.text_area(label='Input your content:')
        if lines.strip() != '':  # Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng nh·∫≠p n·ªôi dung
            flag = True

    # Th·ª±c hi·ªán d·ª± ƒëo√°n n·∫øu c√≥ d·ªØ li·ªáu
    if flag:
        st.write('Content:')
        st.code(lines)  # Hi·ªÉn th·ªã n·ªôi dung ƒë√£ nh·∫≠p ho·∫∑c t·∫£i l√™n
        result = analyze_file(
            lines=lines,
            emoji_dict=emoji_dict,
            teen_dict=teen_dict,
            wrong_lst=wrong_lst,
            stopwords=stopwords_lst,
            phrases_to_join=phrases_to_join,
            word_nev=word_nev,
            word_pov=word_pov,
            model=loaded_model_random
        )

        st.code('New prediction (1: Ti√™u c·ª±c, 5: T√≠ch c·ª±c)')
        st.dataframe(result)  # Hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n
